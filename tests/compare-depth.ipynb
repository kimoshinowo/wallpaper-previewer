{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "import gluoncv\n",
    "# using cpu\n",
    "ctx = mx.cpu(0)\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/KuangHaofei/GluonCV_Test/master/monodepthv2/tutorials/test_img.png'\n",
    "filename = 'test_img.png'\n",
    "# gluoncv.utils.download(url, filename, True)\n",
    "\n",
    "import PIL.Image as pil\n",
    "img = pil.open(filename).convert('RGB')\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "original_width, original_height = img.size\n",
    "feed_height = 192\n",
    "feed_width = 640\n",
    "\n",
    "img = img.resize((feed_width, feed_height), pil.LA)\n",
    "\n",
    "model = gluoncv.model_zoo.get_model('monodepth2_resnet18_kitti_stereo_640x192',\n",
    "                pretrained_base=False, ctx=ctx, pretrained=True)\n",
    "\n",
    "outputs = model.predict(img)\n",
    "disp = outputs[(\"disp\", 0)]\n",
    "disp_resized = mx.nd.contrib.BilinearResize2D(disp, height=original_height, width=original_width)\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "disp_resized_np = disp_resized.squeeze().as_in_context(mx.cpu()).asnumpy()\n",
    "vmax = np.percentile(disp_resized_np, 95)\n",
    "normalizer = mpl.colors.Normalize(vmin=disp_resized_np.min(), vmax=vmax)\n",
    "mapper = cm.ScalarMappable(norm=normalizer, cmap='magma')\n",
    "colormapped_im = (mapper.to_rgba(disp_resized_np)[:, :, :3] * 255).astype(np.uint8)\n",
    "im = pil.fromarray(colormapped_im)\n",
    "im.save('test_output.png')\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "disp_map = mpimg.imread('test_output.png')\n",
    "plt.imshow(disp_map)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import PIL.Image as pil\n",
    "\n",
    "# image = pil.open('images/inputs/rooms/real/sofa.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GLPNImageProcessor, GLPNForDepthEstimation\n",
    "\n",
    "processor = GLPNImageProcessor.from_pretrained(\"vinvino02/glpn-nyu\")\n",
    "model = GLPNForDepthEstimation.from_pretrained(\"vinvino02/glpn-nyu\")\n",
    "\n",
    "import os\n",
    "\n",
    "folder_dir = \"C:/Users/Kim/Documents/Bath/Dissertation/Code/images/inputs/rooms/real\"\n",
    "\n",
    "for image in os.listdir(folder_dir):\n",
    "    # prepare image for the model\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predicted_depth = outputs.predicted_depth\n",
    "\n",
    "    # interpolate to original size\n",
    "    prediction = torch.nn.functional.interpolate(\n",
    "        predicted_depth.unsqueeze(1),\n",
    "        size=image.size[::-1],\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "\n",
    "    # visualize the prediction\n",
    "    output = prediction.squeeze().cpu().numpy()\n",
    "    formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
    "    depth = pil.fromarray(formatted)\n",
    "    plt.imsave('../images/outputs/final-outputs/depth/glpn/' + image, depth)\n",
    "    # plt.imshow(depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPTForDepthEstimation, DPTFeatureExtractor\n",
    "\n",
    "model = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-hybrid-midas\")\n",
    "feature_extractor = DPTFeatureExtractor.from_pretrained(\"Intel/dpt-hybrid-midas\")\n",
    "\n",
    "import os\n",
    "\n",
    "folder_dir = \"C:/Users/Kim/Documents/Bath/Dissertation/Code/images/inputs/rooms/real\"\n",
    "\n",
    "for image in os.listdir(folder_dir):\n",
    "    # prepare image for the model\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predicted_depth = outputs.predicted_depth\n",
    "\n",
    "    # interpolate to original size\n",
    "    prediction = torch.nn.functional.interpolate(\n",
    "        predicted_depth.unsqueeze(1),\n",
    "        size=image.size[::-1],\n",
    "        mode=\"bicubic\"\n",
    "    )\n",
    "\n",
    "    # visualize the prediction\n",
    "    output = prediction.squeeze().cpu().numpy()\n",
    "    formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
    "    depth = pil.fromarray(formatted)\n",
    "    plt.imsave('../images/outputs/final-outputs/depth/midas/' + image, depth)\n",
    "# plt.imshow(depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPTImageProcessor, DPTForDepthEstimation\n",
    "\n",
    "processor = DPTImageProcessor.from_pretrained(\"Intel/dpt-large\")\n",
    "model = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-large\")\n",
    "\n",
    "# prepare image for the model\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predicted_depth = outputs.predicted_depth\n",
    "\n",
    "# interpolate to original size\n",
    "prediction = torch.nn.functional.interpolate(\n",
    "    predicted_depth.unsqueeze(1),\n",
    "    size=image.size[::-1],\n",
    "    mode=\"bicubic\",\n",
    "    align_corners=False,\n",
    ")\n",
    "\n",
    "# visualize the prediction\n",
    "output = prediction.squeeze().cpu().numpy()\n",
    "formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
    "depth = pil.fromarray(formatted)\n",
    "plt.imsave('images/outputs/depth-dpt-large.png', depth)\n",
    "plt.imshow(depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoImageProcessor, DPTForDepthEstimation\n",
    "\n",
    "# image_processor = AutoImageProcessor.from_pretrained(\"facebook/dpt-dinov2-small-nyu\")\n",
    "# model = DPTForDepthEstimation.from_pretrained(\"facebook/dpt-dinov2-small-nyu\")\n",
    "\n",
    "# # prepare image for the model\n",
    "# inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n",
    "#     predicted_depth = outputs.predicted_depth\n",
    "\n",
    "# # interpolate to original size\n",
    "# prediction = torch.nn.functional.interpolate(\n",
    "#     predicted_depth.unsqueeze(1),\n",
    "#     size=image.size[::-1],\n",
    "#     mode=\"bicubic\",\n",
    "#     align_corners=False,\n",
    "# )\n",
    "\n",
    "# # visualize the prediction\n",
    "# output = prediction.squeeze().cpu().numpy()\n",
    "# formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
    "# depth = pil.fromarray(formatted)\n",
    "# plt.imsave('images/outputs/depth-dpt-dinov2-small-nyu.png', depth)\n",
    "# plt.imshow(depth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipeline1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
